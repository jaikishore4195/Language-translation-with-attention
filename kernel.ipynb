{
  "cells": [
    {
      "metadata": {
        "_uuid": "5c339af362b72dedbd0a0a16e8cf77bc04b1fd73",
        "trusted": false
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport tensorflow as tf\ntf.enable_eager_execution()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b295f4de29bef5b6f6cd1351f4a978e29e4fc88a",
        "trusted": false
      },
      "cell_type": "code",
      "source": "import random\nimport nltk",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2044a7eec51bdf86683d64fa1a4a5d474d800597",
        "trusted": false
      },
      "cell_type": "code",
      "source": "lines=open('../input/deu.txt', encoding='utf-8', errors='ignore').read().split('\\n')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5319089c092faae1c7b43635d1dcfb600ea29121",
        "trusted": false
      },
      "cell_type": "code",
      "source": "pairs = [line.split('\\t') for line in  lines]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2d4bbbd957677a6c528b42d2f61cc193ed528c1a",
        "trusted": false
      },
      "cell_type": "code",
      "source": "pairs=pairs[0:-1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "10fdd94a204bfa6d2cf319cb0385050acbf1f333",
        "trusted": false
      },
      "cell_type": "code",
      "source": "questions=[]\nanswers=[]\nfor i in range(0, len(pairs)):\n    questions.append(pairs[i][1])\n    answers.append(pairs[i][0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "af824e78363653b9e6ecdefed325133927e16492",
        "trusted": false
      },
      "cell_type": "code",
      "source": "data=questions+answers",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2f97a872a199387491594e36c35420bf9e892a50",
        "trusted": false
      },
      "cell_type": "code",
      "source": "for i in range(0,len(data)):\n    data[i]=data[i].lower()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "81ae92c49ca46fff8b07444a544469ac8d208a1e",
        "trusted": false
      },
      "cell_type": "code",
      "source": "import re\nfor i in range(0,len(data)):\n    data[i]=re.sub(r'\\d+','',data[i])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8fbb268005b58dc536dd2ffff6bccc2172b4329c",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from nltk.tokenize import RegexpTokenizer",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4443d806ebe301566659463df67e125a5f259836",
        "trusted": false
      },
      "cell_type": "code",
      "source": "tokenizer=RegexpTokenizer(r'\\w+')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cd104322ee4a5d2d1a7d4624644d93617dd08dea",
        "trusted": false
      },
      "cell_type": "code",
      "source": "for i in range(0,len(data)):\n    data[i]=tokenizer.tokenize(data[i])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8d8447b4a15084a1c30aa8e1ad561f92a6c16e11",
        "trusted": false
      },
      "cell_type": "code",
      "source": "ques=[]\nans=[]\nfor i in range(0,len(data)):\n    if i<len(questions):\n        ques.append(data[i][:13])\n    else:\n        ans.append(data[i][:13])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "66f82da24450ed5fe79c8b9ace69c99e00322e15"
      },
      "cell_type": "code",
      "source": "ques = ques[:8000]\nans = ans[:8000]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "22764426aa922de0783daf63d067094e86ad6509",
        "trusted": false
      },
      "cell_type": "code",
      "source": "for i in range(len(ques)):\n    ques[i] =  (9-len(ques[i])) * ['<pad>'] + ques[i]\n    ans[i] = ['<start>'] + ans[i] + ['<end>'] + (7 - len(ans[i])) * ['<pad>']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "169fafa6269270e230100fd0c6a47d02ebcffcd4",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from gensim.models import Word2Vec\n\nw2v_enc=Word2Vec(sentences=ques,min_count=1,size=50,iter=50,window = 3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "756e421950263bc1ee766841e8c82abea0f1b2aa",
        "trusted": false
      },
      "cell_type": "code",
      "source": "w2v_dec=Word2Vec(sentences=ans,min_count=1,size=50,iter=50,window=3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "73f852951cd31573a3a646816458c1db12f60f3f",
        "trusted": false
      },
      "cell_type": "code",
      "source": "vocab_dec=w2v_dec.wv.vocab\n\nvocab_dec=list(vocab_dec)\nint_to_vocab_dec={}\nfor i in range(0,len(vocab_dec)):\n    int_to_vocab_dec[i]=vocab_dec[i]\nvocab_to_int_dec={}\nfor key,value in int_to_vocab_dec.items():\n    vocab_to_int_dec[value]=key",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4399784fe74f3911e958cc9369fa1fa3fc321bfc",
        "trusted": false
      },
      "cell_type": "code",
      "source": "vocab_enc=w2v_enc.wv.vocab\n\nvocab_enc=list(vocab_enc)\nint_to_vocab_enc={}\nfor i in range(0,len(vocab_enc)):\n    int_to_vocab_enc[i]=vocab_enc[i]\nvocab_to_int_enc={}\nfor key,value in int_to_vocab_enc.items():\n    vocab_to_int_enc[value]=key",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "a733c203b29880aae700da44a9bd84c27c6fbb9a"
      },
      "cell_type": "code",
      "source": "len(vocab_to_int_dec)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a562b8b14cdf895425c4174c83e8580741f7fb42",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(ques,ans,test_size = 0.1,random_state = 1234,shuffle = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "64ce2986df3b0a6b430fbf97b1265ec8aab40e4b",
        "trusted": false
      },
      "cell_type": "code",
      "source": "dec_inp_train = np.zeros([len(y_train),9,50])\ndec_inp_test = np.zeros([len(y_test),9,50])\nfor i in range(len(y_train)):\n    temp = y_train[i].copy()\n    try:\n        temp[temp.index('<end>')] = '<pad>'\n    except ValueError:\n        pass\n    y_train[i] = y_train[i][1:] + ['<pad>']\n    \n    dec_inp_train[i] = w2v_dec.wv[temp]\n    x_train[i] = w2v_enc.wv[x_train[i]]\n\nfor i in range(len(y_test)):\n    temp = y_test[i].copy()\n    try:\n        temp[temp.index('<end>')] = '<pad>'\n    except ValueError:\n        pass\n    y_test[i] = y_test[i][1:] + ['<pad>']\n    \n    dec_inp_test[i] = w2v_dec.wv[temp]\n    x_test[i] = w2v_enc.wv[x_test[i]]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fe8251bc97b6c2b87988da3821321bef8561ed7d",
        "trusted": false
      },
      "cell_type": "code",
      "source": "class attention(tf.keras.Model):\n    def __init__(self):\n        super(tf.keras.Model,self).__init__()\n        self.encoder = tf.keras.layers.Bidirectional(tf.keras.layers.CuDNNLSTM(128,return_sequences=True,return_state=True))\n        self.decoder = tf.keras.layers.CuDNNLSTM(256,return_state=True)\n        self.dense = tf.keras.layers.Dense(400,activation='relu')\n        self.out = tf.keras.layers.Dense(1915)\n        self.attention_dense = tf.keras.layers.Dense(1,activation='tanh')\n        self.attention_softmax = tf.keras.layers.Dense(1,activation='softmax')\n        \n            \n    def encoder_func(self,inp):\n        values,ht1,ct1,ht2,ct2 = self.encoder(inp)\n        ht1 = tf.reshape(ht1[-1],shape=[1,128])\n        ht2 = tf.reshape(ht2[-1],shape=[1,128])\n        ct1 = tf.reshape(ct1[-1],shape=[1,128])\n        ct2 = tf.reshape(ct2[-1],shape=[1,128])\n        \n        ht = tf.concat([ht1,ht2],axis=1)\n        ct = tf.concat([ct1,ct2],axis=1)\n        \n        return values,ht,ct\n        \n    \n    def decoder_func(self,enc_inp,dec_input = None):\n        deco_out = tf.convert_to_tensor(w2v_dec['<start>'],dtype=tf.float32)\n        deco_out = tf.reshape(deco_out,shape=[1,1,50])\n        count = 0\n        value = 0\n        predictions = tf.zeros([1,1915])\n        \n        encoder_states,h_t,c_t = self.encoder_func(enc_inp)\n        \n        if dec_input != None:\n            for i in range(16):\n                for j in range(9):\n                    dec_inp = self.attention_func(h_t,dec_input[i][j],encoder_states[i])\n                    value,h_t,c_t = self.decoder(dec_inp,initial_state= [h_t,c_t])\n                    value = self.dense(value)\n                    value = self.out(value)\n                    predictions = tf.concat([predictions,value],axis=0)\n            predictions = predictions[1:]\n            predictions = tf.reshape(predictions,[-1,9,1915])\n            return predictions\n        else:\n            sentence = []\n            while count < 9 and int_to_vocab_dec[value] != '<end>':\n                dec_inp = self.attention_func(h_t,deco_out,encoder_states[0])\n                value,h_t,c_t = self.decoder(deco_out,initial_state = [h_t,c_t])\n                value = self.dense(value)\n                value = self.out(value)\n                value = tf.nn.softmax(value)\n                value = random.choice(np.argsort(value[0])[-3:])\n                sentence.append(int_to_vocab_dec[value])\n                count += 1\n                deco_out = tf.convert_to_tensor(w2v_dec[int_to_vocab_dec[value]])\n                deco_out = tf.reshape(deco_out,shape=[1,1,50])\n            return sentence[:-1]\n                \n    def attention_func(self,dec_h_t,decoder_out,enc_state):\n        \n        temp = tf.zeros([1,512])\n        for i in range(9):\n            enc_statee=enc_state[i]\n            enc_statee=tf.reshape(enc_statee,(1,-1))\n            temp1=tf.concat([enc_statee,dec_h_t],axis=1)\n            temp=tf.concat([temp,temp1],axis=0)\n        temp=temp[1:]\n        \n        attention_weights = self.attention_dense(temp)\n        attention_weights = self.attention_softmax(attention_weights)\n        \n        context_vector = tf.matmul(tf.transpose(enc_state),attention_weights)\n        decoder_out=tf.reshape(decoder_out,(-1,1))\n        attention_context = tf.concat([decoder_out,context_vector],axis=0)\n        attention_context = tf.reshape(attention_context,(1,1,-1))\n\n        return attention_context     \nmodel = attention()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "35d64ce88d49ca9f1bd43a912835e4060caeb9a5",
        "trusted": false
      },
      "cell_type": "code",
      "source": "optimzer = tf.train.RMSPropOptimizer(learning_rate=0.01)\ndef loss_fun(x,y,z):\n    with tf.GradientTape() as t:\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=z,logits=model.decoder_func(x,y)))\n        grads = t.gradient(loss,model.variables)\n        optimzer.apply_gradients(zip(grads,model.variables))\n    return loss",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6e9b943a564192a20dc43b1ac8a54b35c9b23ac5",
        "trusted": false
      },
      "cell_type": "code",
      "source": "for epoch in range(3):\n    i = 0\n    while i < len(x_train):\n        a = np.array(x_train[i:i+16])\n        b = np.array(dec_inp_train[i:i+16])\n        temp = y_train[i:i+16]\n\n        c = np.zeros([16,9,1915])\n\n        for k in range(16):\n            for j in range(9):\n                c[k][j][vocab_to_int_dec[temp[k][j]]] = 1\n\n        los = loss_fun(tf.convert_to_tensor(a,dtype=tf.float32),tf.convert_to_tensor(b,dtype=tf.float32),c)\n        \n        i = i + 16\n        \n        if i % 128 == 0:\n            score = 0\n            test_temp_enc,test_temp_dec = zip(*random.sample(list(zip(x_test, y_test)), 20))\n            for m in range(20):\n                prediction_sent = model.decoder_func(tf.convert_to_tensor(test_temp_enc[m].reshape([1,9,50]),dtype=tf.float32))\n                actual_sent = test_temp_dec[m][:test_temp_dec[m].index('<end>')]\n                score += nltk.translate.bleu_score.sentence_bleu([actual_sent],prediction_sent)\n            print(\"bleu score when i is: \",i, \" is: \",score/20)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "12033e6de98b77ea38f8efa183dd090d81500d41"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}